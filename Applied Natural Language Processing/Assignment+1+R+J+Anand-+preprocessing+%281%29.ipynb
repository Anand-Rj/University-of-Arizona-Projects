{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Text Acquisition and Pre-processing\n",
    "\n",
    "In this assignment you will practice obtaining, extracting, cleaning and pre-processing text from an online source. The objective is to obtain the text from a web page and generate a **pandas** DataFrame containing the text segmented, tokenized and with different types of linguistic annotations.\n",
    "\n",
    "You will work with the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "     -------------------------------------- 409.5/409.5 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.7/122.7 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "     -------------------------------------- 483.8/483.8 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 182.0/182.0 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 5.4/5.4 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting pydantic-core==2.18.3\n",
      "  Downloading pydantic_core-2.18.3-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.1.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "     -------------------------------------- 152.8/152.8 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, colorama, catalogue, blis, annotated-types, wasabi, srsly, pydantic-core, preshed, language-data, cloudpathlib, typer, pydantic, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.5\n",
      "    Uninstalling colorama-0.4.5:\n",
      "      Successfully uninstalled colorama-0.4.5\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.1 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.2 pydantic-core-2.18.3 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.12.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Text Extraction   - [3 Marks]\n",
    "\n",
    "The text you are going to work with corresponds to the following post from the Food and Agriculture Organization of the United Nations website: [World food prices dip in December](https://www.fao.org/newsroom/detail/world-food-prices-dip-in-december/en).\n",
    "\n",
    "In a more realistic scenario, you should download the html document yourself. This could be done with the following code snippet:\n",
    "\n",
    ">```python\n",
    "import requests\n",
    "URL = \"https://www.fao.org/newsroom/detail/world-food-prices-dip-in-december/en\"\n",
    "page = requests.get(URL)\n",
    "html_content = page.content\n",
    "\n",
    "However, for this assignment, you are provided with the downloaded document. The file`world-food-prices.html` can be found in the same directory as this notebook and it can be opened as a regular text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\" /> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"> <title>\\n\\tWorld food prices dip in December\\n</title> <script src=\"/ScriptResource.axd?d=okuX3IVIBwfJlfEQK32K3hu4wA2qYZOscmtsXGLNMaT1SeSa2ByRKpPz9pkmicdQmLZjrfXbzQg-t-PYtREZ1mv-AHy-XqG8V1C8KEuJc1LwVjfZ2AWtsXusqOzwjxwAkWajaiTob5rdLJ_1Q_rhyISygdJ2WS4kb3-Mf0bSt_7dAdqZ2JnDovQKGlnv0vvH0&amp;t=ffffffffb0940fc0\" type=\"text/javascript\"></script><script src=\"/ScriptResource.axd?d=ePnjFy9PuY6CB3GWMX-b_9Fw4jG3rW51lh6cTRiQ1f_9YOhRVOpDf4gVRQwVzn4JRlDVp-Aj_GWhYCgMY8uVHBZj_w4a27EVOxonvJSMs3yERFILsgdOHu7up3GVU-jExdmK0YWhyY1E0W4ye5rzFrSYUigZQBN7nFt18-5XwfQs2ZTBZ5-Na5q3Phaw58Dx0&amp;t=ffffffffb0940fc0\" type=\"text/javascript\"></script><script src=\"https://cse.google.com/cse.js?cx=018170620143701104933%3Aqq82jsfba7w\" type=\"text/javascript\"></script><link href=\"/ResourcePackages/FAO/assets/dist/css/bootstrap.min.css?v=5.2.0&amp;package=FAO\" rel=\"stylesheet\" type=\"text/css\" /> <link href=\"/ResourcePackages/FAO/assets/dist/css/fao-theme.min.css?v=2.6.6&amp;package=FAO\" rel=\"stylesheet\" type=\"text/css\" /> <!-- Google Tag Manager -->\\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\nnew Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\nj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\n\\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n})(window,document,\\'script\\',\\'d'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"world-food-prices.html\", encoding=\"utf8\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "html_content[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    " As you can see the document contains a lot of html tags as well as some **javascript** code. The text also includes fields that are not of interest, such as the navigation menu of the web page. The goal of the first step in this assignment is to extract only the text from the body of the post.   \n",
    "\n",
    "To do this, you must complete the code for the `extract_text` function. This function should parse the content of the html document using the **BeatifulSoup** library, find the html element containing the text of the body of the post, and extract such text. The body of the post is contained by the element with the following **id**: `\"Contentplaceholder1_C011_Col00\"`. Review the [BeautifullSoup documentation](https://beautiful-soup-4.readthedocs.io/en/latest/index.html) to learn how to perform these steps.\n",
    "\n",
    "\n",
    "The function must return the text extracted of which the first 579 characters should look like this:\n",
    "\n",
    "\n",
    "><pre>'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorld food prices dip in December\\nFAO Food Price Index ends 2022 lower than a year earlier\\n\\n\\n\\n\\n                                A farmer in Sicily carrying wheat seeds.\\n                             \\n\\n©FAO/Giorgio Cosulich \\n\\n\\n\\n\\n06/01/2023\\n\\n\\nRome – The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. The FAO Food Price Index averaged 132.4 points in December, 1.0 percent below its value a year earlier.'</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html_content):\n",
    "# Parsing the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Finding the HTML element with the above specified ID\n",
    "    target_element = soup.find('div', {'id': 'Contentplaceholder1_C011_Col00'})\n",
    "\n",
    "# Extract the text from the target element to extracted_text variable\n",
    "    if target_element:\n",
    "        extracted_text = target_element.get_text(separator='\\n')\n",
    "        return extracted_text\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the Full Extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorld food prices dip in December\\n\\n\\nFAO Food Price Index ends 2022 lower than a year earlier\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                A farmer in Sicily carrying wheat seeds.\\n                            \\n \\n\\n\\n\\n\\n©FAO/Giorgio Cosulich\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n06/01/2023\\n\\n\\n\\n\\n\\n\\nRome –\\n The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. \\nThe \\nFAO Food Price Index\\n averaged 132.4 points in December, 1.0 percent below its value a year earlier. However, for 2022 as a whole, the index, which tracks monthly changes in the international prices of commonly-traded food commodities, averaged 143.7 points, 14.3 percent higher than the average value over 2021. \\n“Calmer food commodity prices are welcome after two very volatile years,” said FAO Chief Economist Maximo Torero. “It is important to remain vigilant and keep a strong focus on mitigating global food insecurity given that world food prices remain at elevated levels, with many staples near record highs, and with prices of rice increasing, and still many risks associated with future supplies” \\nVegetable oil world quotations led the decrease, with the FAO Vegetable Oil Price Index down 6.7 percent from November to reach its lowest level since February 2021. International quotations for palm, soy, rapeseed and sunflowerseed oils all declined in December, driven by subdued global import demand and prospects of seasonally rising soy oil production in South America as well as declining crude oil prices.\\nThe FAO Cereal Price Index decreased 1.9 percent from November. Ongoing harvests in the southern hemisphere boosted wheat exportable supplies, while strong competition from Brazil drove down world maize prices. Conversely, international rice prices rose, buoyed by Asian buying and currency appreciation against the United States dollar for exporting countries. \\nThe FAO Meat Price Index in December dropped by 1.2 percent from November, with lower world prices of bovine and poultry meats outweighing higher pig and ovine meat prices. International bovine meat prices were impacted by lacklustre global demand for medium-term supplies, while more-than-adequate export supplies pushed down poultry meat prices. Pig meat prices rose on the back of strong internal holiday demand, especially in Europe, \\nThe FAO Dairy Price Index increased by 1.2 percent in December, following five months of consecutive declines. Higher international cheese prices, reflecting tightening market conditions, drove the monthly increase in the index, while international quotations for butter and milk powder declined. \\nThe FAO Sugar Price Index also rose, increasing by 2.4 percent from November, mostly due to concerns over the impact of adverse weather conditions on crop yields in India and sugarcane crushing delays in Thailand and Australia. \\nLooking back on 2022\\nAs noted, the FAO Food Price Index average over 2022 was notably higher than the previous year, which on top of large increases in \\n2021\\n catalyzed significant strains and food security concerns for lower-income food-importing countries and the adoption, \\ninspired by FAO\\n, of a “Food Shock Window” lending facility by the \\nInternational Monetary Fund\\n. \\nWorld prices of wheat and maize reached record highs over the year. The average value of the FAO Vegetable Oil Price Index for all of 2022 reached a new record high, while the FAO Dairy Price Index and Meat Price Index marked their highest full-year levels since 1990.\\nMore details are available \\nhere\\n.\\n\\n\\n\\n\\n\\n\\nMore on this topic\\n\\n\\nFAO Food Price Index\\nFAO's most recent Cereal Supply and Demand Brief\\nAMIS: Market Monitor\\nAgricultural Market Information System (AMIS)\\nFAO Markets and Trade\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact\\n\\n\\n\\n\\nChristopher Emsden\\n\\n\\nFAO News and Media (Rome)\\n\\n\\n(+39) 06 570 53291\\n\\n\\n[email\\xa0protected]\\n\\n\\n\\n\\n\\n\\nFAO News and Media\\n\\n\\n\\n\\n(+39) 06 570 53625\\n\\n\\n[email\\xa0protected]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorld food prices dip in December\\n\\n\\nFAO Food Price Index ends 2022 lower than a year earlier\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                A farmer in Sicily carrying wheat seeds.\\n                            \\n \\n\\n\\n\\n\\n©FAO/Giorgio Cosulich\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n06/01/2023\\n\\n\\n\\n\\n\\n\\nRome –\\n The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. \\nThe \\nFAO Food Price Index\\n averaged 132.4 points in December, 1.0 perc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = extract_text(html_content)\n",
    "text[:580]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Text Cleanup  - [3 Marks]\n",
    "\n",
    " The text extracted by `extract_text` is not still ready to use. It contains several newline characters and additional spaces that make the text noisy. In the next step of the assignment, you must complete the code for the function `clean_text`. The function should take the text and delete all those newline characters and extra blank spaces. The function should also add a period to the end of those sentences that do not originally contain it, for example, `World food prices dip in December` or `06/01/2023`.\n",
    "\n",
    "You can solve this exercise using the **Python** built-in [string methods](https://docs.python.org/3.9/library/stdtypes.html?highlight=replace#str), such as `replace`, or by [regular expressions](https://docs.python.org/3.9/library/re.html?highlight=re#module-re).\n",
    "\n",
    "The `extract_text` function must return the cleaned text of which the first 499 characters should look like this:\n",
    "\n",
    ">'World food prices dip in December. FAO Food Price Index ends 2022 lower than a year earlier. A farmer in Sicily carrying wheat seeds. ©FAO/Giorgio Cosulich. 06/01/2023. Rome – The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. The FAO Food Price Index averaged 132.4 points in December, 1.0 percent below its value a year earlier.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Normalize whitespace by joining words with a single space\n",
    "    cleaned_text = ' '.join(text.split())\n",
    "\n",
    "    # Replace multiple newlines with a period and space\n",
    "    cleaned_text = re.sub(r'(\\n\\s*)+', '. ', cleaned_text)\n",
    "\n",
    "    # Ensure there is a period at the end of the cleaned text\n",
    "    if not cleaned_text.endswith('.'):\n",
    "        cleaned_text += '.'\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World food prices dip in December FAO Food Price Index ends 2022 lower than a year earlier A farmer in Sicily carrying wheat seeds. ©FAO/Giorgio Cosulich 06/01/2023 Rome – The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. The FAO Food Price Index averaged 132.4 points in December, 1.0 percent below its value a year earlier. How'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = clean_text(text)\n",
    "cleaned_text[:499]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Pre-processing  - [3 Marks]\n",
    "\n",
    "Once the text has been extracted and cleaned up, the next step you must take is to pre-process it. For this, in this assignment, you are going to use the [spaCy](https://spacy.io/) library. This library is an advanced NLP toolkit that allows to execute various pre-processing steps as well as different NLP tasks. **spaCy** provides trained [pipelines](https://spacy.io/usage/processing-pipelines) for a variety of languages that can be installed as individual **Python** modules and include [linguistic featues](https://spacy.io/usage/linguistic-features) such as:\n",
    "\n",
    "- Sentence Segmentation\n",
    "- Tokenization\n",
    "- Stemming and Lemmatization\n",
    "- Stopwords\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Named Entity Recognition\n",
    "- Word Embeddings\n",
    "\n",
    "In this exercise, you will work with the [English pipeline optimized for CPU](https://spacy.io/models/en#en_core_web_sm) that can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.21.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\rjana\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    " You must complete the code for the `preprocess_text` function. This function takes the text and a **spaCy** pipeline as input and should run that pipeline on the text. The function must return a [Doc](https://spacy.io/api/doc) object. Check the [spaCy 101](https://spacy.io/usage/spacy-101) documentation to learn how to apply the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, nlp):\n",
    "        doc = nlp(text)\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = process_text(cleaned_text, nlp)\n",
    "all(map(doc.has_annotation, [\"LEMMA\", \"POS\", \"ENT_TYPE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Creating a DataFrame  - [3 Marks]\n",
    "\n",
    "In the next exercise, you will create a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) that will contain some of the linguistic annotations from the `Doc` object obtained in the previous step. Loading the data into a `DataFrame` provides some advantages such as a better integration with other **Python** machine learning libraries or the option to save the data in a csv file.\n",
    "\n",
    "The goal is to create a `DataFrame` that contains a row per each token in the `Doc` and the following columns:\n",
    "- *sent_id*: The id of the sentence the token belongs to. It represents the position of the sentence in the `Doc`, starting by 0.\n",
    "- *token_id*: The id of the token. It represents the position of the token in the sentence, starting by 0.\n",
    "- *text*: The original text of the token.\n",
    "- *lemma*: The lemmatization of the token.\n",
    "- *pos*: The part-of-speech of the token.\n",
    "- *ent*: The entity type of the token returned by the Named Entity Recognition component.\n",
    "\n",
    "You must complete the code for the `to_dataframe` function. This function takes the [Doc](https://spacy.io/api/doc) object and must return the `DataFrame` described above. The function should iterate over the sentences in the `Doc` (each sentence is a [Span](https://spacy.io/api/span) object) and, for each sentence, it should iterate over its tokens (each token is a [Token](https://spacy.io/api/token) object). For each token, `to_dataframe` should obtain the values to fill the *text*, *lemma*, *pos* and *ent* columns of the `DataFrame`. For example, the content of the `DataFrame` for the setence with *sent_id* equal to 1, corresponding to the second sentence in the `Doc`, should look like this:\n",
    "\n",
    "|    |   sent_id |   token_id | text    | lemma   | pos   | ent   |\n",
    "|---:|----------:|-----------:|:--------|:--------|:------|:------|\n",
    "|  7 |         1 |          0 | FAO     | FAO     | PROPN | ORG   |\n",
    "|  8 |         1 |          1 | Food    | Food    | PROPN | ORG   |\n",
    "|  9 |         1 |          2 | Price   | Price   | PROPN | ORG   |\n",
    "| 10 |         1 |          3 | Index   | Index   | PROPN | ORG   |\n",
    "| 11 |         1 |          4 | ends    | end     | VERB  |       |\n",
    "| 12 |         1 |          5 | 2022    | 2022    | NUM   | DATE  |\n",
    "| 13 |         1 |          6 | lower   | low     | ADJ   |       |\n",
    "| 14 |         1 |          7 | than    | than    | ADP   |       |\n",
    "| 15 |         1 |          8 | a       | a       | DET   | DATE  |\n",
    "| 16 |         1 |          9 | year    | year    | NOUN  | DATE  |\n",
    "| 17 |         1 |         10 | earlier | early   | ADV   | DATE  |\n",
    "| 18 |         1 |         11 | .       | .       | PUNCT |       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(doc):\n",
    "    data = []\n",
    "\n",
    "    for sent_id, sentence in enumerate(doc.sents):\n",
    "        for token_id, token in enumerate(sentence):\n",
    "            data.append({\n",
    "                'sent_id': sent_id,\n",
    "                'token_id': token_id,\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'ent': token.ent_type_\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>ent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>©</td>\n",
       "      <td>©</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FAO</td>\n",
       "      <td>FAO</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Giorgio</td>\n",
       "      <td>Giorgio</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosulich</td>\n",
       "      <td>Cosulich</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>06/01/2023</td>\n",
       "      <td>06/01/2023</td>\n",
       "      <td>NUM</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Rome</td>\n",
       "      <td>Rome</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>index</td>\n",
       "      <td>index</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>prices</td>\n",
       "      <td>price</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>dipped</td>\n",
       "      <td>dip</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>ninth</td>\n",
       "      <td>ninth</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>consecutive</td>\n",
       "      <td>consecutive</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>month</td>\n",
       "      <td>month</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>December</td>\n",
       "      <td>December</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "      <td>NUM</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>declining</td>\n",
       "      <td>decline</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NUM</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>previous</td>\n",
       "      <td>previous</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>month</td>\n",
       "      <td>month</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>Organization</td>\n",
       "      <td>Organization</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>United</td>\n",
       "      <td>United</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>Nations</td>\n",
       "      <td>Nations</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>FAO</td>\n",
       "      <td>FAO</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>reported</td>\n",
       "      <td>report</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>today</td>\n",
       "      <td>today</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id  token_id          text         lemma    pos      ent\n",
       "25        1         0             ©             ©    ADJ         \n",
       "26        1         1           FAO           FAO  PROPN      ORG\n",
       "27        1         2             /             /    SYM      ORG\n",
       "28        1         3       Giorgio       Giorgio  PROPN      ORG\n",
       "29        1         4      Cosulich      Cosulich  PROPN      ORG\n",
       "30        1         5    06/01/2023    06/01/2023    NUM         \n",
       "31        1         6          Rome          Rome  PROPN      GPE\n",
       "32        1         7             –             –  PUNCT         \n",
       "33        1         8           The           the    DET         \n",
       "34        1         9         index         index   NOUN         \n",
       "35        1        10            of            of    ADP         \n",
       "36        1        11         world         world   NOUN         \n",
       "37        1        12          food          food   NOUN         \n",
       "38        1        13        prices         price   NOUN         \n",
       "39        1        14        dipped           dip   VERB         \n",
       "40        1        15           for           for    ADP         \n",
       "41        1        16           the           the    DET     DATE\n",
       "42        1        17         ninth         ninth    ADJ     DATE\n",
       "43        1        18   consecutive   consecutive    ADJ     DATE\n",
       "44        1        19         month         month   NOUN     DATE\n",
       "45        1        20            in            in    ADP         \n",
       "46        1        21      December      December  PROPN     DATE\n",
       "47        1        22          2022          2022    NUM     DATE\n",
       "48        1        23             ,             ,  PUNCT         \n",
       "49        1        24     declining       decline   VERB         \n",
       "50        1        25            by            by    ADP         \n",
       "51        1        26           1.9           1.9    NUM  PERCENT\n",
       "52        1        27       percent       percent   NOUN  PERCENT\n",
       "53        1        28          from          from    ADP         \n",
       "54        1        29           the           the    DET     DATE\n",
       "55        1        30      previous      previous    ADJ     DATE\n",
       "56        1        31         month         month   NOUN     DATE\n",
       "57        1        32             ,             ,  PUNCT         \n",
       "58        1        33           the           the    DET      ORG\n",
       "59        1        34          Food          Food  PROPN      ORG\n",
       "60        1        35           and           and  CCONJ      ORG\n",
       "61        1        36   Agriculture   Agriculture  PROPN      ORG\n",
       "62        1        37  Organization  Organization  PROPN      ORG\n",
       "63        1        38            of            of    ADP      ORG\n",
       "64        1        39           the           the    DET      ORG\n",
       "65        1        40        United        United  PROPN      ORG\n",
       "66        1        41       Nations       Nations  PROPN      ORG\n",
       "67        1        42             (             (  PUNCT         \n",
       "68        1        43           FAO           FAO  PROPN      ORG\n",
       "69        1        44             )             )  PUNCT         \n",
       "70        1        45      reported        report   VERB         \n",
       "71        1        46         today         today   NOUN     DATE\n",
       "72        1        47             .             .  PUNCT         "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = to_dataframe(doc)\n",
    "df[df.sent_id == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Cutomizing the Tokenizer  - [3 Marks]\n",
    "\n",
    "The default components of a **spaCy** pipeline will not always behave according to the needs of your projects. For example, the default tokenizer of the `en_core_web_sm` pipeline does not always splits dates in `month/day/year` format into `month`, `day` and `year`. This is the case for the sentence with *sent_id* equal to 4 that only includes a date in that format:\n",
    "\n",
    "|    |   sent_id |   token_id | text       | lemma      | pos   | ent   |\n",
    "|---:|----------:|-----------:|:-----------|:-----------|:------|:------|\n",
    "| 32 |         4 |          0 | 06/01/2023 | 06/01/2023 | NUM   |       |\n",
    "| 33 |         4 |          1 | .          | .          | PUNCT |       |\n",
    "\n",
    "The goal of the last exercise of this task is to update the `en_core_web_sm` pipeline with a custom tokenizer that forces the splitting of dates in `month/day/year` format so that the sentence above looks like this:\n",
    "\n",
    "|    |   sent_id |   token_id | text   | lemma   | pos   | ent      |\n",
    "|---:|----------:|-----------:|:-------|:--------|:------|:---------|\n",
    "| 32 |         4 |          0 | 06     | 06      | NUM   | CARDINAL |\n",
    "| 33 |         4 |          1 | /      | /       | SYM   |          |\n",
    "| 34 |         4 |          2 | 01     | 01      | NUM   |          |\n",
    "| 35 |         4 |          3 | /      | /       | SYM   |          |\n",
    "| 36 |         4 |          4 | 2023   | 2023    | NUM   |          |\n",
    "| 37 |         4 |          5 | .      | .       | PUNCT |          |\n",
    "\n",
    "You must complete the code for the `customize_tokenizer` function. The function takes the **spaCy** pipeline as input. It should updated the infixes rules of the tokenizer and return the updated version of the pipeline including the customized tokenizer. The `Tokenizer` must keep the default vocabulary and all the default prefixes, infixes and suffixes rules of the pipeline. You should only update the infixes rules adding a regular expression that captures slash (`/`) characters. The `Tokenizer` should **not** include special cases or rules for token and url matching. Check the [spacy's documentation](https://spacy.io/usage/linguistic-features#native-tokenizers) to learn how to customize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_tokenizer(nlp):\n",
    "    infixes = spacy.util.compile_infix_regex(list(nlp.Defaults.infixes + [r'(?<=[0-9])[\\/]']))\n",
    "    custom_tokenizer = Tokenizer(nlp.vocab, infix_finditer=infixes.finditer)\n",
    "\n",
    "    # Replacing the existing tokenizer in the pipeline with the custom tokenizer\n",
    "    nlp.tokenizer = custom_tokenizer\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>ent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>FAO</td>\n",
       "      <td>FAO</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Cereal</td>\n",
       "      <td>Cereal</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Price</td>\n",
       "      <td>Price</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Index</td>\n",
       "      <td>Index</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>decreased</td>\n",
       "      <td>decrease</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NUM</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>November.</td>\n",
       "      <td>November.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>Ongoing</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>harvests</td>\n",
       "      <td>harvest</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>southern</td>\n",
       "      <td>southern</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>hemisphere</td>\n",
       "      <td>hemisphere</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>boosted</td>\n",
       "      <td>boost</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>wheat</td>\n",
       "      <td>wheat</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>exportable</td>\n",
       "      <td>exportable</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>supplies,</td>\n",
       "      <td>supplies,</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>while</td>\n",
       "      <td>while</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>strong</td>\n",
       "      <td>strong</td>\n",
       "      <td>ADJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>competition</td>\n",
       "      <td>competition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>drove</td>\n",
       "      <td>drive</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>maize</td>\n",
       "      <td>maize</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>prices.</td>\n",
       "      <td>prices.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Conversely,</td>\n",
       "      <td>Conversely,</td>\n",
       "      <td>VERB</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sent_id  token_id         text        lemma    pos      ent\n",
       "248        4         0          The          the    DET         \n",
       "249        4         1          FAO          FAO  PROPN         \n",
       "250        4         2       Cereal       Cereal  PROPN         \n",
       "251        4         3        Price        Price  PROPN         \n",
       "252        4         4        Index        Index  PROPN         \n",
       "253        4         5    decreased     decrease   VERB         \n",
       "254        4         6          1.9          1.9    NUM  PERCENT\n",
       "255        4         7      percent      percent   NOUN  PERCENT\n",
       "256        4         8         from         from    ADP         \n",
       "257        4         9    November.    November.  PROPN      GPE\n",
       "258        4        10      Ongoing      ongoing    ADJ         \n",
       "259        4        11     harvests      harvest   NOUN         \n",
       "260        4        12           in           in    ADP         \n",
       "261        4        13          the          the    DET         \n",
       "262        4        14     southern     southern    ADJ         \n",
       "263        4        15   hemisphere   hemisphere   NOUN         \n",
       "264        4        16      boosted        boost   VERB         \n",
       "265        4        17        wheat        wheat   NOUN         \n",
       "266        4        18   exportable   exportable    ADJ         \n",
       "267        4        19    supplies,    supplies,  PROPN   PERSON\n",
       "268        4        20        while        while  SCONJ         \n",
       "269        4        21       strong       strong    ADJ         \n",
       "270        4        22  competition  competition   NOUN         \n",
       "271        4        23         from         from    ADP         \n",
       "272        4        24       Brazil       Brazil  PROPN      GPE\n",
       "273        4        25        drove        drive   VERB         \n",
       "274        4        26         down         down    ADP         \n",
       "275        4        27        world        world   NOUN         \n",
       "276        4        28        maize        maize   NOUN         \n",
       "277        4        29      prices.      prices.  PROPN         \n",
       "278        4        30  Conversely,  Conversely,   VERB  PRODUCT"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customized_nlp = customize_tokenizer(nlp)\n",
    "doc = process_text(cleaned_text, customized_nlp)\n",
    "df = to_dataframe(doc)\n",
    "df[df.sent_id == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
